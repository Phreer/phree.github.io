<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Phree Dream</title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:4000/</link>
    <description>Phree Dream, about AI, Deep Learning and Life</description>
    <pubDate>Sun, 08 Apr 2018 20:31:04 +0800</pubDate>
    
      <item>
        <title>test math formula</title>
        <link>/2018/04/07/test_math.html</link>
        <guid isPermaLink="true">/2018/04/07/test_math.html</guid>
        <description>&lt;p&gt;我想写一个行内公式 \(e^{i\pi}+1=0\)
我想写一个行间公式&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^x=\sum_{i=0}^{\infty} \frac{1}{i!}x^n&lt;/script&gt;

&lt;h1 id=&quot;这是标题1-heading1&quot;&gt;这是标题1, Heading1&lt;/h1&gt;
&lt;h2 id=&quot;这是标题2-heading2&quot;&gt;这是标题2, Heading2&lt;/h2&gt;
&lt;h3 id=&quot;这是标题3-heading3&quot;&gt;这是标题3, Heading3&lt;/h3&gt;
&lt;h4 id=&quot;这是标题4-heading4&quot;&gt;这是标题4, Heading4&lt;/h4&gt;
&lt;h5 id=&quot;这是标题5-heading5&quot;&gt;这是标题5, Heading5&lt;/h5&gt;
&lt;h6 id=&quot;这是标题六-heading6&quot;&gt;这是标题六 Heading6&lt;/h6&gt;
&lt;h1 id=&quot;这是标题1&quot;&gt;这是标题1&lt;/h1&gt;
&lt;h2 id=&quot;这是标题2&quot;&gt;这是标题2&lt;/h2&gt;
&lt;h3 id=&quot;这是标题3&quot;&gt;这是标题3&lt;/h3&gt;
&lt;h4 id=&quot;这是标题4&quot;&gt;这是标题4&lt;/h4&gt;
&lt;h5 id=&quot;这是标题5&quot;&gt;这是标题5&lt;/h5&gt;
&lt;h6 id=&quot;这是标题六&quot;&gt;这是标题六&lt;/h6&gt;
&lt;h1 id=&quot;这是标题1-1&quot;&gt;这是标题1&lt;/h1&gt;
&lt;h2 id=&quot;这是标题2-1&quot;&gt;这是标题2&lt;/h2&gt;
&lt;h3 id=&quot;这是标题3-1&quot;&gt;这是标题3&lt;/h3&gt;
&lt;h4 id=&quot;这是标题4-1&quot;&gt;这是标题4&lt;/h4&gt;
&lt;h5 id=&quot;这是标题5-1&quot;&gt;这是标题5&lt;/h5&gt;
&lt;h6 id=&quot;这是标题六-1&quot;&gt;这是标题六&lt;/h6&gt;
&lt;h1 id=&quot;这是标题1-2&quot;&gt;这是标题1&lt;/h1&gt;
&lt;h2 id=&quot;这是标题2-2&quot;&gt;这是标题2&lt;/h2&gt;
&lt;h3 id=&quot;这是标题3-2&quot;&gt;这是标题3&lt;/h3&gt;
&lt;h4 id=&quot;这是标题4-2&quot;&gt;这是标题4&lt;/h4&gt;
&lt;h5 id=&quot;这是标题5-2&quot;&gt;这是标题5&lt;/h5&gt;
&lt;h6 id=&quot;这是标题六-2&quot;&gt;这是标题六&lt;/h6&gt;
&lt;h1 id=&quot;这是标题1-3&quot;&gt;这是标题1&lt;/h1&gt;
&lt;h2 id=&quot;这是标题2-3&quot;&gt;这是标题2&lt;/h2&gt;
&lt;h3 id=&quot;这是标题3-3&quot;&gt;这是标题3&lt;/h3&gt;
&lt;h4 id=&quot;这是标题4-3&quot;&gt;这是标题4&lt;/h4&gt;
&lt;h5 id=&quot;这是标题5-3&quot;&gt;这是标题5&lt;/h5&gt;
&lt;h6 id=&quot;这是标题六-3&quot;&gt;这是标题六&lt;/h6&gt;
&lt;h1 id=&quot;这是标题1-4&quot;&gt;这是标题1&lt;/h1&gt;
&lt;h2 id=&quot;这是标题2-4&quot;&gt;这是标题2&lt;/h2&gt;
&lt;h3 id=&quot;这是标题3-4&quot;&gt;这是标题3&lt;/h3&gt;
&lt;h4 id=&quot;这是标题4-4&quot;&gt;这是标题4&lt;/h4&gt;
&lt;h5 id=&quot;这是标题5-4&quot;&gt;这是标题5&lt;/h5&gt;
&lt;h6 id=&quot;这是标题六-4&quot;&gt;这是标题六&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;这是一个引用.
This is a quote.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这是&lt;strong&gt;强调&lt;/strong&gt;, 这是&lt;em&gt;斜体&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;这是代码:&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdio&amp;gt;
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello, world!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这是一个行内代码 &lt;code class=&quot;highlighter-rouge&quot;&gt;print(&quot;Hello, world!&quot;&lt;/code&gt;.
I am in English.&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Apr 2018 00:00:00 +0800</pubDate>
      </item>
    
      <item>
        <title>使用 CNNs 实现的 Mnist 分类模型及 Demo, 正确率 0.996+</title>
        <link>/2018/03/31/mnist_model.html</link>
        <guid isPermaLink="true">/2018/03/31/mnist_model.html</guid>
        <description>&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;99.619% 测试集正确率&lt;/li&gt;
  &lt;li&gt;可视化网络激活层的 feature map&lt;/li&gt;
  &lt;li&gt;拍照识别数字&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;所使用的技术&quot;&gt;所使用的技术&lt;/h2&gt;
&lt;h3 id=&quot;深度学习技术&quot;&gt;深度学习技术&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;He Initializer, 即&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma = \sqrt{\frac{2}{f_{in}+f_{out}}}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f_{in}, f_{out}&lt;/script&gt;分别为输入, 输出神经元的个数.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Residual Block&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;Adam Optimizer&lt;/li&gt;
  &lt;li&gt;Data Augmentation&lt;/li&gt;
  &lt;li&gt;Model Emsemble
为了增强泛化能力, 一共训练了 3 个网络, 将 3 个模型的输出加到一起进行判断. 训练过程使用的算法是 AdaBoost.&lt;/li&gt;
  &lt;li&gt;Capture Image Sharpening
使拍摄得到的图片拟合训练/测试数据集的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;软件开发技术&quot;&gt;软件开发技术&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;demo 程序界面: tkinter&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;所使用的的模型结构&quot;&gt;所使用的的模型结构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/网络结构.svg&quot; alt=&quot;Network Architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation-and-dropout&quot;&gt;Data-augmentation and Dropout&lt;/h2&gt;
&lt;p&gt;在训练过程中, 发现由于使用了比较深的网络, 导致了比较严重的过拟合, 因此考虑使用 Data-augmentation 和 Dropout 来增强泛化能力. 以下是这两种技术对训练过程和泛化性能的影响.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/loss.png&quot; alt=&quot;Loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到, 不使用 Data-augmentation 将会使 loss 迅速减少为 0, 明显过拟合, 此时几乎没有梯度下降. 而使用了 Data-augmentation 以后, loss 会产生震荡, 加入 Dropout 以后, 这种现象更加明显.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Data-augmentation&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Dropout&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Training Set Accuracy&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Test Set Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1.0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.991987&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yes&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.997396&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.994691&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yes&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yes&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.997095&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.994591&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;networks-boosting&quot;&gt;Networks Boosting&lt;/h2&gt;
&lt;p&gt;训练过程中发现模型正确率一般在 0.9955 左右徘徊, 达不到所需要的 0.996, 或者只能偶尔达到 0.996, 因此使用 Boosting 方法, 训练 3 个网络进行判决. 使用的算法为 AdaBoosting.
使用 Boosting 以后每次训练结果都能达到 0.996.&lt;/p&gt;

&lt;h2 id=&quot;capture-image-sharpening&quot;&gt;Capture Image Sharpening&lt;/h2&gt;
&lt;p&gt;在训练完网络以后, 测试了模型的实际应用效果, 发现结果并不太好, 很多情况都会被识别为 8. 观察以后发现这是由于实际应用输入的图片和 Mnist 训练集和测试集的分布有较大的差别.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/mnist_samples.png&quot; alt=&quot;Mnist Samples&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/mnist_histogram.png&quot; alt=&quot;Mnist Histogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到, Mnist 数据集的数据分布主要是在接近 0 和 255 间. 而我们通过摄像头拍摄的数据受限于光照条件等原因, 灰度往往集中在直方图的中间.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/original.png&quot; alt=&quot;Original Image Histogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因此必须修正输入图像的直方图. 开始使用的方法是对图像进行伽马变换(当时数字图像处理课刚好上到图像灰度变换部分), 即&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(r)=cr^\gamma&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; 为输入像素, &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; 为 3.0 ~ 7.0 之间的常数.
进过该变换以后得到&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/augmented.png&quot; alt=&quot;Augmented Photo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样与测试数据集分布更为接近一些, 测试结果也大为改观. 之后数字图像处理课程又学习了直方图规定化处理, 因此之后可以考虑使用这种方法进一步拟合测试数据集.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;h3 id=&quot;主界面&quot;&gt;主界面&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/main_window.jpg&quot; alt=&quot;Main Window&quot; /&gt;&lt;/p&gt;

&lt;p&gt;开启摄像头, 可以进行拍照并截取数字&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/camera.jpg&quot; alt=&quot;Capture Image from Camera&quot; /&gt;&lt;/p&gt;

&lt;p&gt;选择一张图片, 并进行测试&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/detect.jpg&quot; alt=&quot;Detect&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到, 测试结果正确. 本人尝试了一些手写体和印刷体数字, 都能够识别正确.&lt;/p&gt;

&lt;h3 id=&quot;可视化-feature-map&quot;&gt;可视化 feature map&lt;/h3&gt;
&lt;p&gt;直接将每一个 feature map显示出来结果如下所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l002_conv2d/activations/grid_activation.png&quot; alt=&quot;第一次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l003_conv2d/activations/grid_activation.png&quot; alt=&quot;第二次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l004_relu/activations/grid_activation.png&quot; alt=&quot;第一次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l005_conv2d/activations/grid_activation.png&quot; alt=&quot;第三次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l006_relu/activations/grid_activation.png&quot; alt=&quot;第二次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l007_conv2d/activations/grid_activation.png&quot; alt=&quot;第四次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l008_conv2d/activations/grid_activation.png&quot; alt=&quot;第五次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l009_relu/activations/grid_activation.png&quot; alt=&quot;第三次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l010_conv2d/activations/grid_activation.png&quot; alt=&quot;第六次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l012_relu/activations/grid_activation.png&quot; alt=&quot;第四次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l013_conv2d/activations/grid_activation.png&quot; alt=&quot;第七次卷积&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l015_relu/activations/grid_activation.png&quot; alt=&quot;第五次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另一种可视化技术使用论文 Visualize and Understanding Convolutional Neural Networks 中的方法, 即对每一个 feature map 进行(反)激活, unpooling 和 deconvolution. GitHub 上已经有一个比较完善的实现[4], 因此配置以后直接调用即可. 结果如下所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l004_relu/deconvolution/grid_image.png&quot; alt=&quot;第一次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l006_relu/deconvolution/grid_image.png&quot; alt=&quot;第二次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l009_relu/deconvolution/grid_image.png&quot; alt=&quot;第三次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l012_relu/deconvolution/grid_image.png&quot; alt=&quot;第四次激活&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/input_mnist_classifier_v3_3_b1_l015_relu/deconvolution/grid_image.png&quot; alt=&quot;第五次激活&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;分析&quot;&gt;分析&lt;/h3&gt;
&lt;p&gt;网络的确能够提取到图像不同的特征, 但由于 Mnist 数据集分布比较单一, 特征也不太多, 并不需要用太深的网络来处理. 可以看到, feature map 中有很大部分都有非常相似的结果. 使用太深的网络还带来严重的过拟合问题, 如前所述, 必须采样强有力的抗过拟合方法才能改善泛化性能. 因此之后尝试训练了一个比较浅的网络, 正确率为 0.9961939102564102. 网络结构如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-31/网络结构2.svg&quot; alt=&quot;网络结构2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;结果:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Training Set Accuracy&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Test Set Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.997095&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.99369&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.9999&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.98758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.992188&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Emsembled&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.9961939102564102&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;可以看到, 模型性能与深层网络基本相同, 甚至更优. 证明之前的推断正确.&lt;/p&gt;

&lt;h2 id=&quot;其他讨论&quot;&gt;其他讨论&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;实验中比较了 SGD 优化算法和 Adam 优化算法的收敛速度, 发现 Adam 的收敛速度确实快很多, 且震荡现象不明显.&lt;/li&gt;
  &lt;li&gt;实验中还尝试了使用 BatchNorm, 但提升并不大. 猜测是由于 BatchNorm 主要用于提高收敛速度, 而实际收敛速度已经非常快了(大约 30k loss 就能达到一个很低的水平), 因此加入了 BatchNorm 反而降低了模型的性能.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;感想&quot;&gt;感想&lt;/h2&gt;
&lt;p&gt;Mnist 的分类之前虽然尝试过, 但并没有对正确率有太高的要求, 且初次接触, 没有进行太深入的探究. 本次任务要求 0.996 的正确率, 虽然对于深度学习来说拟合 Mnist 数据集并不是一件太难的事情, 但要达到很高的正确率还是需要比较认真考量模型结构, 调整参数. 本次任务过程中我也尽量尝试了之前论文, 博客上看到的技术, 比如 Dropout, BatchNorm, Feature map 可视化技术等等. 所以说这次体验收获还是非常大的.&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Visualize and Understanding Convolutional Neural Networks&lt;/li&gt;
  &lt;li&gt;李航. 统计学习方法&lt;/li&gt;
  &lt;li&gt;Feature Visualizing&lt;/li&gt;
  &lt;li&gt;https://github.com/InFoCusp/tf_cnnvis/&lt;/li&gt;
  &lt;li&gt;https://distill.pub/2017/feature-visualization/&lt;/li&gt;
  &lt;li&gt;http://link.springer.com/10.1007/978-3-319-10590-1_53%5Cnhttp://arxiv.org/abs/1311.2901%5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 31 Mar 2018 00:00:00 +0800</pubDate>
      </item>
    
      <item>
        <title>Windows 平台下 Qt OpenCV 环境的配置</title>
        <link>/2018/03/28/setup_opencv_and_basic_usage.html</link>
        <guid isPermaLink="true">/2018/03/28/setup_opencv_and_basic_usage.html</guid>
        <description>&lt;h2 id=&quot;opencv-简介&quot;&gt;OpenCV 简介&lt;/h2&gt;
&lt;p&gt;OpenCV 是一个开源的计算机视觉库, 实现了很多计算机视觉, 图像处理相关的算法, 如基本的图像操作, 物体检测, 光流等等, 在相关领域有着非常广泛的应用. OpenCV 提供了低层的图像运算功能, 也有高级的算法, 同时提供了 C++, Python, Java 接口.&lt;/p&gt;

&lt;p&gt;当前使用 C++ 接口会更加方便一些, 因为 OpenCV 会实现自动管理内存. 主要体现在使用 Mat 数据结构等.&lt;/p&gt;

&lt;h2 id=&quot;环境配置&quot;&gt;环境配置&lt;/h2&gt;
&lt;p&gt;OpenCV 在 Win 平台的配置还是很简单的, 因为有已经编译好的. 其他平台需要使用 CMake 编译源文件, 请参考其他资料.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;从&lt;a href=&quot;https://opencv.org/releases.html&quot;&gt;官网&lt;/a&gt; 下载对应的版本.&lt;/li&gt;
  &lt;li&gt;Win平台有预编译的版本, 只需要执行 exe 文件将编译的文件解压到对应目录即可. &lt;strong&gt;注意&lt;/strong&gt;: 预编译版本似乎只支持 MSVC 编译器,  因此安装 Qt 的时候尽量选择使用 MSVC 编译器(前提是已经安装 VS). 当然如果已经安装了其他编译器, 也可以在 Qt 的 Tools - &amp;gt; Options -&amp;gt; Build &amp;amp; Run 中设置 MSVC(Qt 会自动检测已安装的编译器). 如图所示&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-3-28/setup_compiler.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解压以后需要在 .pro 文件中添加 lib 文件和 include 目录. &lt;strong&gt;注意&lt;/strong&gt;: 我们这里使用的版本是 OpenCV 3.4, 该版本大对 lib 文件进行了合并, 只有一个 world 文件. 这和网上大多数教程所用的版本不一样. 比如我的 .pro 文件定义是&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;.pro 中添加依赖项的方法是 &lt;code class=&quot;highlighter-rouge&quot;&gt;ITEM += PATH/FILE&lt;/code&gt;, 使用 -L 表示添加目录, -l 表示添加文件. 注意换行需要在结尾添加 \ 符号.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 添加对应的 include 目录
INCLUDEPATH += &quot;C:\Program Files\opencv\build\include&quot; \
&quot;E:\Program Files (x86)\Intel\RSSDK\include&quot; \
&quot;E:\Program Files (x86)\Intel\RSSDK\sample\common\include&quot; \
&quot;C:\Users\Phree\Desktop\c++\record_v0_1\include&quot;
...
# 添加对应的 lib 目录, 前面一大部分是因为不添加的话一直出现 LNK 2019 错误
# 参考了可用的 VS 配置, 把 Microsoft SDK 的文件包含进来才得到解决.
LIBS += -lws2_32 \ # winsock32 lib 文件
-L&quot;C:\Program Files (x86)\Microsoft SDKs\Windows\v7.1A\Lib\x64&quot; \
-lkernel32 \
-luser32 \
-lgdi32 \
-lwinspool \
-lcomdlg32 \
-ladvapi32 \
-lshell32 \
-lole32 \
-loleaut32 \
-luuid \
-lodbc32 \
-lodbccp32 \
-L&quot;C:\Users\Phree\Desktop\c++\record_v0_1\lib&quot; \
-lmyo64

# 对于 OpenCV 和 Realsense SDK, Debug 版本和 Release 版本所需要的链接文件是不一样的.
# 注意到 Debug 版本的结尾是带 _d 的
CONFIG(debug, debug|release): {
LIBS += -L&quot;C:\Program Files\opencv\build\x64\vc14\lib&quot; \
-lopencv_world331d \
-L&quot;E:\Program Files (x86)\Intel\RSSDK\lib\x64&quot; \
-llibpxc_d \
-L&quot;E:\Program Files (x86)\Intel\RSSDK\sample\common\lib\x64\v140&quot; \
-llibpxcutils_d
-llibpxcutilsmd_d
} else:CONFIG(release, debug|release): {
LIBS += -L&quot;C:\Program Files\opencv\build\x64\vc14\lib&quot; \
-lopencv_world331 \
-L&quot;E:\Program Files (x86)\Intel\RSSDK\lib\x64&quot; \
-llibpxc \
-L&quot;E:\Program Files (x86)\Intel\RSSDK\sample\commonlib\x64\v140&quot; \
-llibpxcutils
-llibpxcutilsmd
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;修改 .pro 文件以后执行 Buld -&amp;gt; qmake, 然后再进行编译.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;一个简单的测试程序&quot;&gt;一个简单的测试程序&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include&amp;lt;iostream&amp;gt;  
#include &amp;lt;opencv2/core/core.hpp&amp;gt;  
#include &amp;lt;opencv2/highgui/highgui.hpp&amp;gt;  
  
    
using namespace cv;  
      
        
int main()  {  
    Mat img=imread(&quot;pic.jpg&quot;);  
    namedWindow(&quot;pic&quot;);  
    imshow(&quot;pic&quot;,img);  
    // 等待6000 ms后窗口自动关闭  
    waitKey(6000);  
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果能够正常编译运行, 那恭喜啦, 配置成功了. 如果出现找不到头文件的错误, 则应该是 include path 没有设置好. 如果出现 LNK 2019 错误, 则是 lib 文件没有设置成功.&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Mar 2018 00:00:00 +0800</pubDate>
      </item>
    
      <item>
        <title>语音识别中的 CTC 算法详解</title>
        <link>/2018/03/22/ctc_algorithem.html</link>
        <guid isPermaLink="true">/2018/03/22/ctc_algorithem.html</guid>
        <description>&lt;h2 id=&quot;ctcconnectionist-temporal-classiﬁcation&quot;&gt;CTC(Connectionist Temporal Classiﬁcation)&lt;/h2&gt;
&lt;p&gt;在神经网络训练中用于训练序列化的数据, 比如说语音, 手写文字(虽然手写文字是一张图, 但不同的文字之间可以认为是按一定序列进行输入的, 因为要考虑前后的约束关系). 在数据量剧增的今天, 我们可以非常容易地获取大量语音数据, 但遗憾的是这些语音通常没有非常仔细的标注. 如何李用深度学习和大量数据的优势是语音识别领域的一个难题. CTC 的提出为使用深度神经网络进行语音识别奠定了基础.&lt;/p&gt;
&lt;h3 id=&quot;介绍&quot;&gt;介绍&lt;/h3&gt;
&lt;p&gt;在语音识别领域存在的一个问题是, 我们的训练数据往往只是语音片段和对应的文本, 语音序列和文本长度不同, 并且不是对齐的, 也就是, 我们不知道语音中的哪一帧是属于一个字, 也不知道一个字的长短. 这是因为进行对齐标注是一件非常耗费人力和时间的事情. 并且人与人的语速不同, 导致不可能使用一种强制的规则来强制确定一个字的长短. 这种难以对齐的问题也出现在手写字体识别中, 因为每个字占的大小也是不确定的.
CTC 正是为了解决这个问题而被提出的. CTC 用于把语音经过处理以后得到的特征映射为文本, 而对特征提取的过程并没有限制, 也即是说可以使用任何模型接到 CTC 中得到结果.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ctc/alignment.svg&quot; alt=&quot;alignment&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;形式化问题&quot;&gt;形式化问题&lt;/h4&gt;
&lt;p&gt;CTC 网络将输入划分成固定长度的帧. 我们的目标是对给定符号输出符号集(如汉字表, 字母表)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{L^*}&lt;/script&gt;

&lt;p&gt;和训练数据集, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;, 训练集中每一个样本 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{(x,z)}&lt;/script&gt;包含一个由T个向量组成的 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; 维语音序列&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}=\{x_1, x_2,...,x_T\}&lt;/script&gt;

&lt;p&gt;和输出文本&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{z}=\{z_1,z_2,...,z_U\}&lt;/script&gt;

&lt;p&gt;希望找到映射 
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}=h(\mathbf{x})&lt;/script&gt; .
事实上这个映射并不容易直接得到, CTC 通过一个巧妙的算法来得到这个映射.
CTC 网络先将输入的 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; 维长度为 &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; 向量序列 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; 映射为输出的 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; 维长度为&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;的向量序列 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}=\{\mathbf{y^1, y^2, ..., y^T}\}&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y^t}=\{y^t_1, y^t_2, ..., y^t_m\}&lt;/script&gt;

&lt;p&gt;其中 &lt;script type=&quot;math/tex&quot;&gt;y^t_k&lt;/script&gt; 表示&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;时刻网络的第 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 个输出, 通常解释为输出低 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;个符号的概率.
这个过程表示为
&lt;script type=&quot;math/tex&quot;&gt;\mathscr{N}:(\mathbb{R}^m)^T\mapsto (\mathbb{R}^n)^T&lt;/script&gt;. 
通常由一个 RNN 网络来完成, 如上图所示的 Alignment.
如此我们可以看到, 一个输出 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}&lt;/script&gt;, 可以对应一条路径, 我们将其表示为 &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;. 每一条路径的概率为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\pi|\mathbf{x})=\prod _{t=1}^Ty^t_{\pi_t}, \forall \pi \in L^T&lt;/script&gt;

&lt;p&gt;即一条路径的概率等于路径上各个符号输出概率的乘积(事实上, 这里是假设输出符号之间相互独立).
随后 CTC 需要对于连续相同的输出进行合并. 但是这种方法会出现一些问题, 比如说对于单词 hello, 输出的&lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}&lt;/script&gt;可能是 heelllooo, 进行合并以后会得到 helo 而不是 hello. 为了解决这个问题, CTC 中引入了一个特殊的 &amp;lt; blank &amp;gt; 字符, 最后这个字符是会被移去的. 我们期望它输出 heel&amp;lt; blank &amp;gt;loo, 最后就可以得到所期望的 hello. 使用&amp;lt; blank &amp;gt;还解决了语音中的停顿的问题, 同一段时间没有输出时, 也输出&amp;lt; blank &amp;gt;字符. 这时符号集&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
L'=L\cup &lt;blank&gt; %]]&gt;&lt;/script&gt;我们将这个合并和删除&amp;lt; blank &amp;gt;的过程记为&lt;script type=&quot;math/tex&quot;&gt;\mathcal{B}:L^{'T}\mapsto L^{\le T}&lt;/script&gt;, 将这个过程的输出记为&lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;, 有&lt;script type=&quot;math/tex&quot;&gt;\mathbf{l}=\mathcal{B}(\pi)&lt;/script&gt;显然这是一个多对一映射, 即可能有多个可能的路径对应一种可能的文本输出.
实际上, 我们所希望得到的就是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(\mathbf{x})=\arg \max_{l\in L^{\le T}}p(\mathbf{l|x})&lt;/script&gt;

&lt;p&gt;其中&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{l}|\mathbf{x})=\sum_{\pi}p(\pi|x), \pi \in \{\pi|\mathcal{B}(\pi)=\mathbf{l}\}&lt;/script&gt;

&lt;p&gt;即在所有可能的输出文本序列中找到概率最大的那一个, 而每一个输出文本序列的概率可以通过边沿概率分布公式得到, 即所有可能得到该输出文本的路径的概率之和.&lt;/p&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt;
&lt;p&gt;事实上直接优化得到上式也不是一件容易的事情. 因为这意味着我们需要遍历所有的路径来得到各个可能输出文本的概率. CTC 论文中提到两种方法可以简化这个步骤.
第一种方法比较简单, 我们可以简单地认为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(\mathbf{x})\approx \mathcal{B}(\pi^*)&lt;/script&gt;

&lt;p&gt;其中,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^*=\arg \max _{\pi\in N^t}p(\pi|\mathbf{x})&lt;/script&gt;

&lt;p&gt;直观上理解就是直接找概率最大的路径, 认为其对应的输出一般也是概率最大的. 这样我们其实只需要选择每个&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;中出现概率最大的符号. 可以解释为希望找一个成绩最好的班级, 但查看每一个学生的成绩又太过于繁琐, 于是只看成绩最好的学生的成绩.
从上面的例子可以看出, 这种方法事实上过于简化了, 很多情况下我们可能会遇到这种情况. 于是论文中还提出了前缀搜索解码(Prefix Search Decoding), 这种方法的结果普遍好于第一种方法.
事实上如果我们考量所有可能的路径, 我们会发现其实有很多路径是重复计算的, 有很多路径也是明显不可能的, 这启发我们使用动态规划和剪枝来去除冗余计算和淘汰不可能的路径. Prefix Search Decoding 算法类似于 HMM 模型中的前向-后向算法(F-B Algorithm).
我们首先定义&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_t(x)\equiv \sum_{\mathcal{B}(\pi_{1:t})=\mathbf{1:s}}\prod_{t'=1}^{t}y^{t'}_{\pi_{t'}}&lt;/script&gt;

&lt;p&gt;解释为经过&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;步, 得到输出文本序列&lt;script type=&quot;math/tex&quot;&gt;\mathbf{l}_{1:s}&lt;/script&gt;的概率.
从而我们可以得到递推关系
…未完待续…&lt;/p&gt;

&lt;p&gt;CTC 假设在给定输入的情况下, 输出是条件独立的(事实上在一些情况下该假设是不成立的, 因此这也是 CTC 的一个主要缺点). 根据条件概率公式, 可以把它分解为. 每一种输出序列都可以看成是一条路径, 可以将其视为计算不同路径概率. 如果我们遍历所有路径计算概率, 计算量将是非常巨大的, 举个例子, 假设输出有 10 种不同的符号, 一个语音序列被分拆成 40 帧, 那么可能的路径将是 &lt;script type=&quot;math/tex&quot;&gt;10^{40}&lt;/script&gt;! 这是不可接受的. 幸运的是, 经过一些分析我们可以发现这种暴力算法中包含了很多冗余的计算, 熟悉算法的朋友可能已经发现了, 可以通过动态规划来高效地完成这一计算.
对于每一帧, 输出的概率分布 &lt;script type=&quot;math/tex&quot;&gt;P(a_t|X)&lt;/script&gt; 通常用 RNN 来实现.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Mar 2018 00:00:00 +0800</pubDate>
      </item>
    
      <item>
        <title>Summary of Batchnorm Algorithm</title>
        <link>/2018/02/26/bn2.html</link>
        <guid isPermaLink="true">/2018/02/26/bn2.html</guid>
        <description>&lt;h1 id=&quot;batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/h1&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;每一层输入的分布改变(covariate shift) -&amp;gt; 学习速率不能太快, 参数初始化需要很小心
使用BN -&amp;gt; 改善以上问题, 同时可以减少对Dropout的依赖.
文中对internal covariate shift的定义是&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;the change in the distribution of network activations due to the change in net- work parameters during training.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实covariate shift也可以用频域调整(domain adaptation)的方法缓解.
考虑一个例子, 假设网络某一层为sigmoid层, 有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=g(Wu+b)&lt;/script&gt;

&lt;p&gt;其中 &lt;script type=&quot;math/tex&quot;&gt;g(x)=\frac{1}{1+exp(-x)}&lt;/script&gt; 知道 &lt;script type=&quot;math/tex&quot;&gt;g'(x)&lt;/script&gt; 随着&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;增大而减小, 因此如果某一层的结果使得 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; 分布在较大的区域, 则容易出现梯度饱和. 而且这种效应随着网络深度的增加, 很容易被扩大. 其实这个问题也可以通过使用ReLU来解决.&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;Algorithm BN comes as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; Values of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; over a mini-batch: &lt;script type=&quot;math/tex&quot;&gt;\mathcal{B}=\{x_{1...m}\}&lt;/script&gt;;
Parameters to be learned: &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\{y_i=BN_{\gamma,\beta}(x_i)\}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  &amp; \mu_{\mathcal{B}}=\frac{1}{m}\sum_{i=1}^{m}x_i \\
  &amp; \sigma_{\mathcal{B}}^2=\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2 \\
  &amp; \hat{x_{i}}=\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}} \\
  &amp; y_i=\gamma\hat{x_i}+\beta \equiv BN_{\gamma,\beta}(x_i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Seems to be very easy. As we can see, the internal variable (we can cosider it as output of a layer) $\hat{x_i}$ has 0 mean and 1 variance, which makes it work. Later, we use $\beta$ and $\gamma$ to reconstruct the data, preserving the presence capability of the network.&lt;/p&gt;

&lt;h2 id=&quot;inference-network&quot;&gt;Inference Network&lt;/h2&gt;
&lt;p&gt;BN algorithm is used for accelerating training process, so it’s unnecessary during inference (or generation). We use &lt;script type=&quot;math/tex&quot;&gt;E[X] \leftarrow E[\mu_{\mathcal{B}}]&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Var[x] \leftarrow \frac{m}{m-1}E_{\mathcal{B}}[\sigma_{\mathcal{B}}^2]&lt;/script&gt; to estimate the mean and variance of the input.&lt;/p&gt;

&lt;h2 id=&quot;convolutional-networks-case&quot;&gt;Convolutional Networks Case&lt;/h2&gt;
&lt;p&gt;When using batchnorm in convolutional networks, we should note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;different feature maps are batch normalized seperately and have their own parameters(&lt;script type=&quot;math/tex&quot;&gt;\beta^{(k)}, \gamma^{(k)}&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;different locations in a single feature map are jointly normalized.
    &lt;h2 id=&quot;something-more&quot;&gt;Something More&lt;/h2&gt;
    &lt;p&gt;In the original paper, the authors added batchnorm before an activation, and analyze based on this fact. However, some other researchers’ experiment argued that batchnorm after activation seems to have better performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;理论依据&quot;&gt;理论依据&lt;/h2&gt;
&lt;p&gt;LeCun曾经证明过, 如果输入是经过白化(Whitened, 就是均值为0, 方差为1)网络会收敛得更快.&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Feb 2018 00:00:00 +0800</pubDate>
      </item>
    
      <item>
        <title>这是我的第一篇博文, 用于测试</title>
        <link>/2018/02/18/first-article.html</link>
        <guid isPermaLink="true">/2018/02/18/first-article.html</guid>
        <description>&lt;p&gt;大年初三, 祝大家新年快乐!&lt;/p&gt;

&lt;p&gt;建立这个博客的目的是分享在学习过程中的心得, 认识一些志同道合的朋友. 为了建立这个博客还是花了不少时间呢, 希望能够用心维护. 虽然现在还很简陋, 但是以后会慢慢改进的!&lt;/p&gt;
</description>
        <pubDate>Sun, 18 Feb 2018 00:00:00 +0800</pubDate>
      </item>
    
  </channel>
</rss>