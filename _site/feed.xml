<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Phree Dream</title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:4000/</link>
    <description>Phree Dream, about AI, Deep Learning and Life</description>
    <pubDate>Tue, 27 Feb 2018 00:35:28 +0800</pubDate>
    
      <item>
        <title>Summary of Batchnorm Algorithm</title>
        <link>/2018/02/26/bn2.html</link>
        <guid isPermaLink="true">/2018/02/26/bn2.html</guid>
        <description>&lt;h1 id=&quot;batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/h1&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;每一层输入的分布改变(covariate shift) -&amp;gt; 学习速率不能太快, 参数初始化需要很小心
使用BN -&amp;gt; 改善以上问题, 同时可以减少对Dropout的依赖.
文中对internal covariate shift的定义是&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;the change in the distribution of network activations due to the change in net- work parameters during training.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实covariate shift也可以用频域调整(domain adaptation)的方法缓解.
考虑一个例子, 假设网络某一层为sigmoid层, 有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=g(Wu+b)&lt;/script&gt;

&lt;p&gt;其中 &lt;script type=&quot;math/tex&quot;&gt;g(x)=\frac{1}{1+exp(-x)}&lt;/script&gt; 知道 &lt;script type=&quot;math/tex&quot;&gt;g'(x)&lt;/script&gt; 随着&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;增大而减小, 因此如果某一层的结果使得 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; 分布在较大的区域, 则容易出现梯度饱和. 而且这种效应随着网络深度的增加, 很容易被扩大. 其实这个问题也可以通过使用ReLU来解决.&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;Algorithm BN comes as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; Values of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; over a mini-batch: &lt;script type=&quot;math/tex&quot;&gt;\mathcal{B}=\{x_{1...m}\}&lt;/script&gt;;
Parameters to be learned: &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\{y_i=BN_{\gamma,\beta}(x_i)\}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  &amp; \mu_{\mathcal{B}}=\frac{1}{m}\sum_{i=1}^{m}x_i \\
  &amp; \sigma_{\mathcal{B}}^2=\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2 \\
  &amp; \hat{x_{i}}=\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}} \\
  &amp; y_i=\gamma\hat{x_i}+\beta \equiv BN_{\gamma,\beta}(x_i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Seems to be very easy. As we can see, the internal variable (we can cosider it as output of a layer) $\hat{x_i}$ has 0 mean and 1 variance, which makes it work. Later, we use $\beta$ and $\gamma$ to reconstruct the data, preserving the presence capability of the network.&lt;/p&gt;

&lt;h2 id=&quot;inference-network&quot;&gt;Inference Network&lt;/h2&gt;
&lt;p&gt;BN algorithm is used for accelerating training process, so it’s unnecessary during inference (or generation). We use &lt;script type=&quot;math/tex&quot;&gt;E[X] \leftarrow E[\mu_{\mathcal{B}}]&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Var[x] \leftarrow \frac{m}{m-1}E_{\mathcal{B}}[\sigma_{\mathcal{B}}^2]&lt;/script&gt; to estimate the mean and variance of the input.&lt;/p&gt;

&lt;h2 id=&quot;convolutional-networks-case&quot;&gt;Convolutional Networks Case&lt;/h2&gt;
&lt;p&gt;When using batchnorm in convolutional networks, we should note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;different feature maps are batch normalized seperately and have their own parameters(&lt;script type=&quot;math/tex&quot;&gt;\beta^{(k)}, \gamma^{(k)}&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;different locations in a single feature map are jointly normalized.
    &lt;h2 id=&quot;something-more&quot;&gt;Something More&lt;/h2&gt;
    &lt;p&gt;In the original paper, the authors added batchnorm before an activation, and analyze based on this fact. However, some other researchers’ experiment argued that batchnorm after activation seems to have better performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;理论依据&quot;&gt;理论依据&lt;/h2&gt;
&lt;p&gt;LeCun曾经证明过, 如果输入是经过白化(Whitened, 就是均值为0, 方差为1)网络会收敛得更快.&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Feb 2018 00:00:00 +0800</pubDate>
      </item>
    
      <item>
        <title>这是我的第一篇博文, 用于测试</title>
        <link>/2018/02/18/first-article.html</link>
        <guid isPermaLink="true">/2018/02/18/first-article.html</guid>
        <description>&lt;p&gt;大年初三, 祝大家新年快乐!&lt;/p&gt;

&lt;p&gt;建立这个博客的目的是分享在学习过程中的心得, 认识一些志同道合的朋友. 为了建立这个博客还是花了不少时间呢, 希望能够用心维护. 虽然现在还很简陋, 但是以后会慢慢改进的!&lt;/p&gt;
</description>
        <pubDate>Sun, 18 Feb 2018 00:00:00 +0800</pubDate>
      </item>
    
  </channel>
</rss>