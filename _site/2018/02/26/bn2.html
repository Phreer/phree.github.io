<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <!--Baidu tracker verification-->
    <meta name="baidu-site-verification" content="dOp9xdAi2j" />    
    <!-- JQuery (used for bootstrap and jekyll search) -->
    <script src="/assets/js/jquery-3.2.1.min.js" ></script>
    
    <!-- Main JS (navbar.js and katex_init.js)-->
    <script defer=true src="/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- Canonical -->
    <link rel="canonical" href="http://localhost:4000/2018/02/26/bn2.html">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="Phree Dream" href="http://localhost:4000///feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/assets/css/font-awesome.min.css">

    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    

    
    <!-- Mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-110451933-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Summary of Batchnorm Algorithm</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Summary of Batchnorm Algorithm" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Motivation 每一层输入的分布改变(covariate shift) -&gt; 学习速率不能太快, 参数初始化需要很小心 使用BN -&gt; 改善以上问题, 同时可以减少对Dropout的依赖. 文中对internal covariate shift的定义是 the change in the distribution of network activations due to the change in net- work parameters during training. 其实covariate shift也可以用频域调整(domain adaptation)的方法缓解. 考虑一个例子, 假设网络某一层为sigmoid层, 有 其中 知道 随着增大而减小, 因此如果某一层的结果使得 分布在较大的区域, 则容易出现梯度饱和. 而且这种效应随着网络深度的增加, 很容易被扩大. 其实这个问题也可以通过使用ReLU来解决. Batch Normalization Algorithm BN comes as follows: Input: Values of over a mini-batch: ; Parameters to be learned: , ; Output: Seems to be very easy. As we can see, the internal variable (we can cosider it as output of a layer) $\hat{x_i}$ has 0 mean and 1 variance, which makes it work. Later, we use $\beta$ and $\gamma$ to reconstruct the data, preserving the presence capability of the network. Inference Network BN algorithm is used for accelerating training process, so it’s unnecessary during inference (or generation). We use , to estimate the mean and variance of the input. Convolutional Networks Case When using batchnorm in convolutional networks, we should note: different feature maps are batch normalized seperately and have their own parameters() different locations in a single feature map are jointly normalized. Something More In the original paper, the authors added batchnorm before an activation, and analyze based on this fact. However, some other researchers’ experiment argued that batchnorm after activation seems to have better performance. 理论依据 LeCun曾经证明过, 如果输入是经过白化(Whitened, 就是均值为0, 方差为1)网络会收敛得更快." />
<meta property="og:description" content="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Motivation 每一层输入的分布改变(covariate shift) -&gt; 学习速率不能太快, 参数初始化需要很小心 使用BN -&gt; 改善以上问题, 同时可以减少对Dropout的依赖. 文中对internal covariate shift的定义是 the change in the distribution of network activations due to the change in net- work parameters during training. 其实covariate shift也可以用频域调整(domain adaptation)的方法缓解. 考虑一个例子, 假设网络某一层为sigmoid层, 有 其中 知道 随着增大而减小, 因此如果某一层的结果使得 分布在较大的区域, 则容易出现梯度饱和. 而且这种效应随着网络深度的增加, 很容易被扩大. 其实这个问题也可以通过使用ReLU来解决. Batch Normalization Algorithm BN comes as follows: Input: Values of over a mini-batch: ; Parameters to be learned: , ; Output: Seems to be very easy. As we can see, the internal variable (we can cosider it as output of a layer) $\hat{x_i}$ has 0 mean and 1 variance, which makes it work. Later, we use $\beta$ and $\gamma$ to reconstruct the data, preserving the presence capability of the network. Inference Network BN algorithm is used for accelerating training process, so it’s unnecessary during inference (or generation). We use , to estimate the mean and variance of the input. Convolutional Networks Case When using batchnorm in convolutional networks, we should note: different feature maps are batch normalized seperately and have their own parameters() different locations in a single feature map are jointly normalized. Something More In the original paper, the authors added batchnorm before an activation, and analyze based on this fact. However, some other researchers’ experiment argued that batchnorm after activation seems to have better performance. 理论依据 LeCun曾经证明过, 如果输入是经过白化(Whitened, 就是均值为0, 方差为1)网络会收敛得更快." />
<link rel="canonical" href="http://localhost:4000/2018/02/26/bn2.html" />
<meta property="og:url" content="http://localhost:4000/2018/02/26/bn2.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-26T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Motivation 每一层输入的分布改变(covariate shift) -&gt; 学习速率不能太快, 参数初始化需要很小心 使用BN -&gt; 改善以上问题, 同时可以减少对Dropout的依赖. 文中对internal covariate shift的定义是 the change in the distribution of network activations due to the change in net- work parameters during training. 其实covariate shift也可以用频域调整(domain adaptation)的方法缓解. 考虑一个例子, 假设网络某一层为sigmoid层, 有 其中 知道 随着增大而减小, 因此如果某一层的结果使得 分布在较大的区域, 则容易出现梯度饱和. 而且这种效应随着网络深度的增加, 很容易被扩大. 其实这个问题也可以通过使用ReLU来解决. Batch Normalization Algorithm BN comes as follows: Input: Values of over a mini-batch: ; Parameters to be learned: , ; Output: Seems to be very easy. As we can see, the internal variable (we can cosider it as output of a layer) $\\hat{x_i}$ has 0 mean and 1 variance, which makes it work. Later, we use $\\beta$ and $\\gamma$ to reconstruct the data, preserving the presence capability of the network. Inference Network BN algorithm is used for accelerating training process, so it’s unnecessary during inference (or generation). We use , to estimate the mean and variance of the input. Convolutional Networks Case When using batchnorm in convolutional networks, we should note: different feature maps are batch normalized seperately and have their own parameters() different locations in a single feature map are jointly normalized. Something More In the original paper, the authors added batchnorm before an activation, and analyze based on this fact. However, some other researchers’ experiment argued that batchnorm after activation seems to have better performance. 理论依据 LeCun曾经证明过, 如果输入是经过白化(Whitened, 就是均值为0, 方差为1)网络会收敛得更快.","@type":"BlogPosting","url":"http://localhost:4000/2018/02/26/bn2.html","headline":"Summary of Batchnorm Algorithm","dateModified":"2018-02-26T00:00:00+08:00","datePublished":"2018-02-26T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/02/26/bn2.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Manual seo tags -->
    <!--
    <title>Summary of Batchnorm Algorithm | Phree Dream</title>
    <meta name="description" content="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftMotivation每一层输入的分布改变(covariate shift) -&gt; 学习速率不能太快, 参数初始化需要很小心使...">
    -->
</head>

  <body>
    <header class="site-header">
    
    <!-- Logo and title -->
	<div class="branding">
		<a href="/">
			<img class="avatar" src="/assets/img/triangle.svg" alt=""/>
		</a>

		<h1 class="site-title">
			<a href="/">Phree Dream</a>
		</h1>
	</div>
    
    <!-- Toggle menu -->
    <nav class="clear">
    <a id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>
    
    <!-- Menu -->
    <ul>
        
        
        
        
        <li>
            <a class="clear" href="/about/">
                关于
            </a>
        </li>
        
        
        
        <li>
            <a class="clear" href="/archives/">
                归档
            </a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
         
        
        
        <li>
            <a class="clear" href="http://localhost:4000/search">
                <i class="fa fa-search" aria-hidden="true"></i>
            </a>
        </li>
        
        
        <li>
            <a class="clear" href="http://localhost:4000/tags">
                <i class="fa fa-tags" aria-hidden="true"></i>
            </a>
        </li>
        
        
    </ul>
        
	</nav>
</header>

    <div class="content">
      <article >
  <header id="main" style="background-image: url('/')">
    <h1 id="Summary+of+Batchnorm+Algorithm" class="title">Summary of Batchnorm Algorithm</h1>
    <p class="meta">
    Feb 26, 2018
    
    </p>
  </header>
  <div class="colms">
  <div class="colms-md-three-forth">
    <section class="post-content"><h1 id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1>
<h2 id="motivation">Motivation</h2>
<p>每一层输入的分布改变(covariate shift) -&gt; 学习速率不能太快, 参数初始化需要很小心
使用BN -&gt; 改善以上问题, 同时可以减少对Dropout的依赖.
文中对internal covariate shift的定义是</p>

<blockquote>
  <p>the change in the distribution of network activations due to the change in net- work parameters during training.</p>
</blockquote>

<p>其实covariate shift也可以用频域调整(domain adaptation)的方法缓解.
考虑一个例子, 假设网络某一层为sigmoid层, 有</p>

<script type="math/tex; mode=display">z=g(Wu+b)</script>

<p>其中 <script type="math/tex">g(x)=\frac{1}{1+exp(-x)}</script> 知道 <script type="math/tex">g'(x)</script> 随着<script type="math/tex">x</script>增大而减小, 因此如果某一层的结果使得 <script type="math/tex">x</script> 分布在较大的区域, 则容易出现梯度饱和. 而且这种效应随着网络深度的增加, 很容易被扩大. 其实这个问题也可以通过使用ReLU来解决.</p>

<h2 id="batch-normalization">Batch Normalization</h2>
<p>Algorithm BN comes as follows:</p>

<p><strong>Input:</strong> Values of <script type="math/tex">x</script> over a mini-batch: <script type="math/tex">\mathcal{B}=\{x_{1...m}\}</script>;
Parameters to be learned: <script type="math/tex">\gamma</script>, <script type="math/tex">\beta</script>;</p>

<p><strong>Output:</strong> <script type="math/tex">\{y_i=BN_{\gamma,\beta}(x_i)\}</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
  & \mu_{\mathcal{B}}=\frac{1}{m}\sum_{i=1}^{m}x_i \\
  & \sigma_{\mathcal{B}}^2=\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2 \\
  & \hat{x_{i}}=\frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}} \\
  & y_i=\gamma\hat{x_i}+\beta \equiv BN_{\gamma,\beta}(x_i)
\end{aligned} %]]></script>

<p>Seems to be very easy. As we can see, the internal variable (we can cosider it as output of a layer) $\hat{x_i}$ has 0 mean and 1 variance, which makes it work. Later, we use $\beta$ and $\gamma$ to reconstruct the data, preserving the presence capability of the network.</p>

<h2 id="inference-network">Inference Network</h2>
<p>BN algorithm is used for accelerating training process, so it’s unnecessary during inference (or generation). We use <script type="math/tex">E[X] \leftarrow E[\mu_{\mathcal{B}}]</script>, <script type="math/tex">Var[x] \leftarrow \frac{m}{m-1}E_{\mathcal{B}}[\sigma_{\mathcal{B}}^2]</script> to estimate the mean and variance of the input.</p>

<h2 id="convolutional-networks-case">Convolutional Networks Case</h2>
<p>When using batchnorm in convolutional networks, we should note:</p>
<ul>
  <li>different feature maps are batch normalized seperately and have their own parameters(<script type="math/tex">\beta^{(k)}, \gamma^{(k)}</script>)</li>
  <li>different locations in a single feature map are jointly normalized.
    <h2 id="something-more">Something More</h2>
    <p>In the original paper, the authors added batchnorm before an activation, and analyze based on this fact. However, some other researchers’ experiment argued that batchnorm after activation seems to have better performance.</p>
  </li>
</ul>

<h2 id="理论依据">理论依据</h2>
<p>LeCun曾经证明过, 如果输入是经过白化(Whitened, 就是均值为0, 方差为1)网络会收敛得更快.</p>
</section>
    <section class="post-tag">
    
    


    <footer>
        <div class="tag-list">
        
          <div class="meta">Tags</div>
        
            
        
          <a class="button" href="/tags#
    算法">
            <p><i class="fa fa-tag fa-fw"></i> 
    算法</p>
          </a>
        
          <a class="button" href="/tags#深度学习
    ">
            <p><i class="fa fa-tag fa-fw"></i> 深度学习
    </p>
          </a>
        
        </div>
    </footer>
  

    </section>
  </div>

  <div class="colms-md-one-forth"><div id="post-directory-module" class="mobile-hidden">
    <section class="post-directory">

    <dl></dl>
    </section>
</div>

<script src="/assets/js/jquery.toc.js"></script>
</div>
  </div>
    <!-- Tag list -->
   
</article>

<!-- Disqus -->

<div class="comments">
  <div id="disqus_thread"></div>
<script type="text/javascript">
	var disqus_shortname = 'phree-dream-1';
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>启用JavaScript以查看评论.</noscript>

</div>


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <p>上一篇</p>
      <a href="/2018/02/18/first-article.html">
        这是我的第一篇博文, 用于测试
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <p>下一篇</p>
      <a href="/2018/03/22/ctc_algorithem.html">
        语音识别中的 CTC 算法详解
      </a>
  </div>
  
</div>


    </div>
    
<footer class="site-footer">
    <p class="text">Powered by <a href="https://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/sylhare/Type-on-Strap">Type on Strap</a>
</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                
<li>
	<a href="http://localhost:4000////feed.xml" title="订阅RSS">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>



<li>
	<a href="mailto:iphreeliu@gmail.com" title="邮箱">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>













<li>
	<a href="https://github.com/phreer" title="关注 GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>
































                </ul>
            </div>
</footer>




  </body>
</html>
